{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-25T15:35:19.612998Z","iopub.status.busy":"2022-04-25T15:35:19.612681Z","iopub.status.idle":"2022-04-25T15:35:21.662719Z","shell.execute_reply":"2022-04-25T15:35:21.661983Z","shell.execute_reply.started":"2022-04-25T15:35:19.612921Z"},"trusted":true},"outputs":[],"source":["import sys\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:35:48.222850Z","iopub.status.busy":"2022-04-25T15:35:48.222318Z","iopub.status.idle":"2022-04-25T15:36:43.333764Z","shell.execute_reply":"2022-04-25T15:36:43.333106Z","shell.execute_reply.started":"2022-04-25T15:35:48.222810Z"},"trusted":true},"outputs":[],"source":["\n","main_df = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\", dtype={\"article_id\": str})\n","print('Training data shape:', main_df.shape)\n","main_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Data pre-processing"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:36:43.335719Z","iopub.status.busy":"2022-04-25T15:36:43.335449Z","iopub.status.idle":"2022-04-25T15:38:13.582190Z","shell.execute_reply":"2022-04-25T15:38:13.581472Z","shell.execute_reply.started":"2022-04-25T15:36:43.335683Z"},"trusted":true},"outputs":[],"source":["main_df[\"t_dat\"] = pd.to_datetime(main_df[\"t_dat\"])\n","\n","# Set only articles purchased in the past 2 years as active\n","active_articles = main_df.groupby(\"article_id\")[\"t_dat\"].max().reset_index()\n","active_articles = active_articles[active_articles[\"t_dat\"] >= \"2019-09-01\"].reset_index()\n","print('Active transactions shape: ', active_articles.shape)\n","\n","# Retain only rows with active articles\n","\n","main_df = main_df[main_df[\"article_id\"].isin(active_articles[\"article_id\"])].reset_index(drop=True)\n","print('Current shape of main_df: ', main_df.shape)\n","\n","# Extract the week a transaction occured (assumed last week as week 0)\n","main_df[\"week\"] = (main_df[\"t_dat\"].max() - main_df[\"t_dat\"]).dt.days // 7\n","\n","# Label encode the article_id \n","article_ids = np.concatenate([[\"placeholder\"], np.unique(main_df[\"article_id\"].values)])\n","le = LabelEncoder()\n","le.fit(article_ids)\n","main_df[\"article_id\"] = le.transform(main_df[\"article_id\"])\n","\n","batch_size = 256\n","SEQ_LEN = 16\n","SEED = 0\n","num_workers = 4"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:38:13.601241Z","iopub.status.busy":"2022-04-25T15:38:13.598863Z","iopub.status.idle":"2022-04-25T15:38:13.619382Z","shell.execute_reply":"2022-04-25T15:38:13.618675Z","shell.execute_reply.started":"2022-04-25T15:38:13.601202Z"},"trusted":true},"outputs":[],"source":["main_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Model definition on PyTorch"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:38:13.623191Z","iopub.status.busy":"2022-04-25T15:38:13.622804Z","iopub.status.idle":"2022-04-25T15:38:16.946434Z","shell.execute_reply":"2022-04-25T15:38:16.945713Z","shell.execute_reply.started":"2022-04-25T15:38:13.623156Z"},"trusted":true},"outputs":[],"source":["class RecSysModel(nn.Module):\n","    def __init__(self, article_shape):\n","        super(RecSysModel, self).__init__()\n","        \n","        # Each input sample defines the purchase of a single article.\n","        # We are training our model to predict this article based on the purchase history (input to model).\n","        # Purchase history is defined by a list (maximum number defined by seq_len) of articles recently purchased by a customer \n","        # upto k weeks before current purchased article \n","        \n","        # Create embedding vector for this purchase history\n","        self.artic_embedding = nn.Embedding(article_shape[0], embedding_dim=article_shape[1])\n","        self.artic_likelihood = nn.Parameter(torch.zeros(article_shape[0]), requires_grad=True)\n","        \n","        # NN model using 1-D convolutions. Train this neural network to predict the current purchased article.\n","        self.top = nn.Sequential(nn.Conv1d(3, 32, kernel_size=1), nn.LeakyReLU(), nn.BatchNorm1d(32),\n","                                 nn.Conv1d(32, 8, kernel_size=1), nn.LeakyReLU(), nn.BatchNorm1d(8),\n","                                 nn.Conv1d(8, 1, kernel_size=1))\n","    def forward(self, inputs):\n","        article_hist, week_hist = inputs[0], inputs[1]\n","        \n","        x = self.artic_embedding(article_hist)\n","        x = F.normalize(x, dim=2)\n","        \n","        # (bs, seq_len, 72582)\n","        x = x@F.normalize(self.artic_embedding.weight).T\n","        \n","        x, indices = x.max(axis=1)\n","        \n","        # (bs, 1, 72582)\n","        x = x.clamp(1e-3, 0.999)\n","        \n","        # inverse sigmoid - converts probabilities to a real valued num.\n","        x = -torch.log(1/x - 1)\n","        \n","        max_week = week_hist.unsqueeze(2).repeat(1, 1, x.shape[-1]).gather(1, indices.unsqueeze(1).repeat(1, week_hist.shape[1], 1))\n","        max_week = max_week.mean(axis=1).unsqueeze(1)\n","        \n","        x = torch.cat([x.unsqueeze(1), max_week, self.artic_likelihood[None, None, :].repeat(x.shape[0], 1, 1)], axis=1)\n","        x = self.top(x).squeeze(1)\n","        \n","        return x\n","    \n","    \n","model = RecSysModel((len(le.classes_), 512))\n","model = model.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["## PyTorch Dataset Class Definition"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:38:16.947987Z","iopub.status.busy":"2022-04-25T15:38:16.947753Z","iopub.status.idle":"2022-04-25T15:38:24.555050Z","shell.execute_reply":"2022-04-25T15:38:24.554304Z","shell.execute_reply.started":"2022-04-25T15:38:16.947955Z"},"trusted":true},"outputs":[],"source":["# Create the PyTorch dataset class\n","class RecSysDataset(Dataset):\n","    def __init__(self, main_df, weeks, seq_len, train=True, is_test=False, create_dataset=True):\n","        \n","        # Maximum number of weeks for creating purchase history\n","        self.max_week_hist = 5\n","        \n","        # Create this purchase history dataframe \n","        if create_dataset:\n","            if train:\n","                self.weeks = list(range(1, weeks))\n","                df = pd.concat([self.create_dataset(main_df, w) for w in self.weeks]).reset_index(drop=True)\n","            else:\n","                self.weeks = [0]\n","                df = pd.concat([self.create_dataset(main_df, w) for w in self.weeks]).reset_index(drop=True)\n","\n","            self.df = df.reset_index(drop=True)\n","            self.seq_len = seq_len\n","            self.is_test = is_test\n","        \n","        else:\n","            self.df = main_df.reset_index(drop=True)\n","            self.seq_len = seq_len\n","            self.is_test = is_test\n","            \n","    \n","    def create_dataset(self, df, week):\n","        hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + self.max_week_hist)]\n","        hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n","        hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n","\n","        target_df = df[df[\"week\"] == week]\n","        target_df = target_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n","        target_df.rename(columns={\"article_id\": \"target\"}, inplace=True)\n","        target_df[\"week\"] = week\n","\n","        return target_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","    \n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        # Target is a one-hot vector defining the current article purchased\n","        if self.is_test:\n","            target = torch.zeros(2).float()\n","        else:\n","            target = torch.zeros(len(article_ids)).float()\n","            for t in row.target:\n","                target[t] = 1.0\n","        \n","        # Upto seq_len article_ids can be stored in purchase history\n","        article_hist = torch.zeros(self.seq_len).long()\n","        \n","        # The corresponding weeks for each article in the history \n","        week_hist = torch.ones(self.seq_len).float()\n","        \n","        \n","        if isinstance(row.article_id, list):\n","            if len(row.article_id) >= self.seq_len:\n","                article_hist = torch.LongTensor(row.article_id[-self.seq_len:])\n","                week_hist = (torch.LongTensor(row.week_history[-self.seq_len:]) - row.week)/self.max_week_hist/2\n","            else:\n","                article_hist[-len(row.article_id):] = torch.LongTensor(row.article_id)\n","                week_hist[-len(row.article_id):] = (torch.LongTensor(row.week_history) - row.week)/self.max_week_hist/2\n","                \n","        return article_hist, week_hist, target\n","    \n","RecSysDataset(main_df, 5, 64, train=False, is_test=False, create_dataset=True)[1]"]},{"cell_type":"markdown","metadata":{},"source":["## Training  and Validatation"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:38:24.556429Z","iopub.status.busy":"2022-04-25T15:38:24.556180Z","iopub.status.idle":"2022-04-25T15:38:24.571824Z","shell.execute_reply":"2022-04-25T15:38:24.571153Z","shell.execute_reply.started":"2022-04-25T15:38:24.556379Z"},"trusted":true},"outputs":[],"source":["# Model training function\n","def train(model, train_loader, val_loader, criterion, optimizer, epochs):\n","\n","    np.random.seed(SEED)\n","    scaler = torch.cuda.amp.GradScaler()\n","    train_losses = []\n","    val_losses = []\n","    val_maps = []\n","    \n","    for e in range(epochs):\n","        \n","        model.train()\n","        tbar = tqdm(train_loader, file=sys.stdout)\n","        lr = lr_schedule(optimizer, e)\n","\n","        for input_data in tbar:\n","            \n","            # Fetch inputs\n","            inputs = tuple(d.cuda() for d in input_data[:-1])\n","            target = input_data[-1].cuda()\n","\n","            optimizer.zero_grad()\n","            \n","            # Forward pass and calculate loss\n","            with torch.cuda.amp.autocast():\n","                model_out = model(inputs)\n","                loss = criterion(model_out, target) + dice_loss(model_out, target)\n","            \n","            # Update model weights\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            \n","            train_losses.append(loss.detach().cpu().item())\n","            avg_loss = np.round(100*np.mean(train_losses), 4)\n","            tbar.set_description(f\"Epoch: {e+1}; lr: {lr}; Loss: {avg_loss}\")\n","            \n","        val_map, val_loss = validate(model, val_loader, criterion)\n","        val_losses.extend(val_loss)\n","        val_maps.append(val_map)\n","        log_text = f\"Epoch {e+1}\\nTrain Loss: {avg_loss}\\nValidation MAP: {val_map}\\n\"\n","        print(log_text)\n","        \n","    return model, train_losses, val_losses, val_maps\n","\n","# Model validation\n","def validate(model, val_loader, criterion, k=12):\n","\n","    model.eval()\n","    tbar = tqdm(val_loader, file=sys.stdout)\n","    maps = []\n","    val_losses = []\n","    \n","    with torch.no_grad():\n","        for input_data in tbar:\n","\n","            inputs = tuple(d.cuda() for d in input_data[:-1])\n","            target = input_data[-1].cuda()\n","\n","            model_out = model(inputs)\n","            loss = criterion(model_out, target) + dice_loss(model_out, target)\n","            val_losses.append(loss.detach().cpu().item())\n","            _, indices = torch.topk(model_out, k, dim=1)\n","\n","            indices = indices.detach().cpu().numpy()\n","            target = target.detach().cpu().numpy()\n","\n","            for i in range(indices.shape[0]):\n","                maps.append(calc_metric(indices[i], target[i]))\n","        \n","    \n","    return np.mean(maps), val_losses"]},{"cell_type":"markdown","metadata":{},"source":["### Helper Functions"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:38:24.573438Z","iopub.status.busy":"2022-04-25T15:38:24.572903Z","iopub.status.idle":"2022-04-25T15:38:24.586472Z","shell.execute_reply":"2022-04-25T15:38:24.585722Z","shell.execute_reply.started":"2022-04-25T15:38:24.573400Z"},"trusted":true},"outputs":[],"source":["def dice_loss(y_pred, y_true):\n","\n","    y_pred = y_pred.sigmoid()\n","    intersect = (y_true*y_pred).sum(axis=1)\n","    return 1 - (intersect/(intersect + y_true.sum(axis=1) + y_pred.sum(axis=1))).mean()\n","\n","# Learning rate schedule\n","def lr_schedule(optimizer, epoch):\n","\n","    if epoch < 1:\n","        lr = 5e-5\n","    elif epoch < 6:\n","        lr = 1e-3\n","    elif epoch < 9:\n","        lr = 1e-4\n","    else:\n","        lr = 1e-5\n","\n","    for param in optimizer.param_groups:\n","        param['lr'] = lr\n","        \n","    return lr\n","\n","# Adam Optimizer definition\n","def get_optimizer(model):\n","    \n","    optim = torch.optim.Adam(filter(lambda param: param.requires_grad, model.parameters()), lr=3e-4, betas=(0.9, 0.999),eps=1e-08)\n","    return optim\n","\n","# Calculate mAP metric on the validation set\n","def calc_metric(topk_preds, target_array, k=12):\n","    \n","    map_met = []\n","    tp, fp = 0, 0\n","    for pred in topk_preds:\n","        if target_array[pred]:\n","            tp += 1\n","            map_met.append(tp/(tp + fp))\n","        else:\n","            fp += 1\n","            \n","    return np.sum(map_met) / min(k, target_array.sum())"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T15:38:24.588421Z","iopub.status.busy":"2022-04-25T15:38:24.587919Z","iopub.status.idle":"2022-04-25T17:31:07.795670Z","shell.execute_reply":"2022-04-25T17:31:07.794754Z","shell.execute_reply.started":"2022-04-25T15:38:24.588384Z"},"trusted":true},"outputs":[],"source":["MODEL_NAME = \"exp001\"\n","\n","# Dataloaders for training and validation datasets\n","val_dataset = RecSysDataset(main_df, 0, SEQ_LEN, train=False, is_test=False)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n","                          pin_memory=False, drop_last=False)\n","\n","train_dataset = RecSysDataset(main_df, weeks=5, seq_len=SEQ_LEN, train=True, is_test=False)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n","                          pin_memory=False, drop_last=True)\n","\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = get_optimizer(model)\n","model, train_losses, val_losses, val_maps = train(model, train_loader, val_loader, criterion, optimizer, epochs=10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T19:22:44.099431Z","iopub.status.busy":"2022-04-25T19:22:44.099131Z","iopub.status.idle":"2022-04-25T19:22:44.104382Z","shell.execute_reply":"2022-04-25T19:22:44.103706Z","shell.execute_reply.started":"2022-04-25T19:22:44.099396Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T19:27:44.012356Z","iopub.status.busy":"2022-04-25T19:27:44.012097Z","iopub.status.idle":"2022-04-25T19:27:44.275044Z","shell.execute_reply":"2022-04-25T19:27:44.274301Z","shell.execute_reply.started":"2022-04-25T19:27:44.012328Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(12, 8))\n","ax.plot(val_maps)\n","ax.set_title(label='Validation Score', fontsize=16)\n","ax.set_xlabel('Epochs', fontsize=16)\n","ax.set_ylabel('Score', fontsize=16)\n","plt.savefig('validation_score.png')\n","plt.show()"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T19:30:53.236063Z","iopub.status.busy":"2022-04-25T19:30:53.235774Z","iopub.status.idle":"2022-04-25T19:30:53.493133Z","shell.execute_reply":"2022-04-25T19:30:53.492477Z","shell.execute_reply.started":"2022-04-25T19:30:53.236035Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(12, 8))\n","ax.plot(val_losses)\n","ax.set_title(label='Training loss across epochs', fontsize=16)\n","ax.set_xlabel('Steps', fontsize=16)\n","ax.set_ylabel('Loss', fontsize=16)\n","plt.savefig('val_loss.png')\n","plt.show()"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-04-25T19:30:28.076083Z","iopub.status.busy":"2022-04-25T19:30:28.075537Z","iopub.status.idle":"2022-04-25T19:30:28.375835Z","shell.execute_reply":"2022-04-25T19:30:28.375177Z","shell.execute_reply.started":"2022-04-25T19:30:28.076044Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(12, 8))\n","# ax.plot(val_losses)\n","ax.plot(train_losses)\n","ax.set_title(label='Training loss across epochs', fontsize=16)\n","ax.set_xlabel('Steps', fontsize=16)\n","ax.set_ylabel('Loss', fontsize=16)\n","plt.savefig('train_loss.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Finetune along with Validation Data"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T22:15:53.536114Z","iopub.status.busy":"2022-04-23T22:15:53.534629Z","iopub.status.idle":"2022-04-24T00:07:06.027688Z","shell.execute_reply":"2022-04-24T00:07:06.026869Z","shell.execute_reply.started":"2022-04-23T22:15:53.536067Z"},"trusted":true},"outputs":[],"source":["train_dataset = RecSysDataset(main_df[main_df[\"week\"] < 4].append(val_df), 5, SEQ_LEN, train=True, is_test=False, create_dataset=False)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n","                          pin_memory=False, drop_last=True)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = get_optimizer(model)\n","\n","model = train(model, train_loader, val_loader, criterion, optimizer, epochs=10)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T16:06:06.356183Z","iopub.status.busy":"2022-04-24T16:06:06.355874Z","iopub.status.idle":"2022-04-24T16:06:10.763022Z","shell.execute_reply":"2022-04-24T16:06:10.762294Z","shell.execute_reply.started":"2022-04-24T16:06:06.356150Z"},"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv').drop(\"prediction\", axis=1)\n","print('Test dataframe shape: ', test_df.shape)\n","test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Inference"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T00:07:10.418001Z","iopub.status.busy":"2022-04-24T00:07:10.415818Z","iopub.status.idle":"2022-04-24T00:07:18.011370Z","shell.execute_reply":"2022-04-24T00:07:18.010604Z","shell.execute_reply.started":"2022-04-24T00:07:10.417959Z"},"trusted":true},"outputs":[],"source":["# For inference, gather article purchase history (past 5 weeks) for each customer id on sample_submission.csv\n","# Use this to make predictions\n","def create_test_dataset(test_df):\n","    max_week_hist = 5\n","    \n","    week = -1\n","    test_df[\"week\"] = week \n","    hist_df = main_df[(main_df[\"week\"] > week) & (main_df[\"week\"] <= week + max_week_hist)]\n","    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n","    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n","    \n","    return test_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n","\n","test_df = create_test_dataset(test_df)\n","test_df.head()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T00:07:18.095636Z","iopub.status.busy":"2022-04-24T00:07:18.095275Z","iopub.status.idle":"2022-04-24T00:52:30.446042Z","shell.execute_reply":"2022-04-24T00:52:30.445127Z","shell.execute_reply.started":"2022-04-24T00:07:18.095598Z"},"trusted":true},"outputs":[],"source":["test_ds = RecSysDataset(test_df, 0, SEQ_LEN, train=False, is_test=True, create_dataset=False)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n","                          pin_memory=False, drop_last=False)\n","\n","# Inference pipeline\n","def inference(model, loader, k=12):\n","    model.eval()\n","    tbar = tqdm(loader, file=sys.stdout)\n","    \n","    prediction_list = []\n","    \n","    with torch.no_grad():\n","        for input_data in tbar:\n","            inputs = tuple(d.cuda() for d in input_data[:-1])\n","            target = input_data[-1].cuda()\n","\n","            model_out = model(inputs)\n","            _, indices = torch.topk(model_out, k, dim=1)\n","\n","            indices = indices.detach().cpu().numpy()\n","            target = target.detach().cpu().numpy()\n","\n","            for i in range(indices.shape[0]):\n","                prediction_list.append(\" \".join(list(le.inverse_transform(indices[i]))))\n","        \n","    \n","    return prediction_list\n","\n","test_df[\"prediction\"] = inference(model, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Save Outputs"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T00:52:30.448353Z","iopub.status.busy":"2022-04-24T00:52:30.447894Z","iopub.status.idle":"2022-04-24T00:52:42.525163Z","shell.execute_reply":"2022-04-24T00:52:42.524215Z","shell.execute_reply.started":"2022-04-24T00:52:30.448307Z"},"trusted":true},"outputs":[],"source":["test_df.to_csv(\"output.csv\", index=False, columns=[\"customer_id\", \"prediction\"])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
