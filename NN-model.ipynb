{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-24T14:22:25.324850Z","iopub.status.busy":"2022-04-24T14:22:25.324537Z","iopub.status.idle":"2022-04-24T14:22:27.258261Z","shell.execute_reply":"2022-04-24T14:22:27.257523Z","shell.execute_reply.started":"2022-04-24T14:22:25.324767Z"},"trusted":true},"outputs":[],"source":["import sys\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T14:22:27.259956Z","iopub.status.busy":"2022-04-24T14:22:27.259675Z","iopub.status.idle":"2022-04-24T14:23:25.234359Z","shell.execute_reply":"2022-04-24T14:23:25.233721Z","shell.execute_reply.started":"2022-04-24T14:22:27.259919Z"},"trusted":true},"outputs":[],"source":["\n","main_df = pd.read_csv(\"transactions_train.csv\", dtype={\"article_id\": str})\n","print('Training data shape:', main_df.shape)\n","main_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Data pre-processing"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T14:36:08.431649Z","iopub.status.busy":"2022-04-24T14:36:08.431363Z","iopub.status.idle":"2022-04-24T14:37:39.985807Z","shell.execute_reply":"2022-04-24T14:37:39.985039Z","shell.execute_reply.started":"2022-04-24T14:36:08.431609Z"},"trusted":true},"outputs":[],"source":["main_df[\"t_dat\"] = pd.to_datetime(main_df[\"t_dat\"])\n","\n","# Set only articles purchased in the past 2 years as active\n","active_articles = main_df.groupby(\"article_id\")[\"t_dat\"].max().reset_index()\n","active_articles = active_articles[active_articles[\"t_dat\"] >= \"2019-09-01\"].reset_index()\n","print('Active transactions shape: ', active_articles.shape)\n","\n","# Retain only rows with active articles\n","\n","main_df = main_df[main_df[\"article_id\"].isin(active_articles[\"article_id\"])].reset_index(drop=True)\n","print('Current shape of main_df: ', main_df.shape)\n","\n","# Extract the week a transaction occured (assumed last week as week 0)\n","main_df[\"week\"] = (main_df[\"t_dat\"].max() - main_df[\"t_dat\"]).dt.days // 7\n","\n","# Label encode the article_id \n","article_ids = np.concatenate([[\"placeholder\"], np.unique(main_df[\"article_id\"].values)])\n","le = LabelEncoder()\n","le.fit(article_ids)\n","main_df[\"article_id\"] = le.transform(main_df[\"article_id\"])\n","\n","num_workers = 8\n","batch_size = 256\n","SEQ_LEN = 16\n","SEED = 0"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T16:19:43.623109Z","iopub.status.busy":"2022-04-24T16:19:43.622691Z","iopub.status.idle":"2022-04-24T16:19:43.643462Z","shell.execute_reply":"2022-04-24T16:19:43.642500Z","shell.execute_reply.started":"2022-04-24T16:19:43.623064Z"},"trusted":true},"outputs":[],"source":["main_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Model definition on PyTorch"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T14:56:09.789383Z","iopub.status.busy":"2022-04-24T14:56:09.788881Z","iopub.status.idle":"2022-04-24T14:56:12.605008Z","shell.execute_reply":"2022-04-24T14:56:12.604229Z","shell.execute_reply.started":"2022-04-24T14:56:09.789344Z"},"trusted":true},"outputs":[],"source":["class RecSysModel(nn.Module):\n","    def __init__(self, article_shape):\n","        super(RecSysModel, self).__init__()\n","        \n","        # Each input sample defines the purchase of a single article.\n","        # We are training our model to predict this article based on the purchase history (input to model).\n","        # Purchase history is defined by a list (maximum number defined by seq_len) of articles recently purchased by a customer \n","        # upto k weeks before current purchased article \n","        \n","        # Create embedding vector for this purchase history\n","        self.artic_embedding = nn.Embedding(article_shape[0], embedding_dim=article_shape[1])\n","        self.artic_likelihood = nn.Parameter(torch.zeros(article_shape[0]), requires_grad=True)\n","        \n","        # NN model using 1-D convolutions. Train this neural network to predict the current purchased article.\n","        self.top = nn.Sequential(nn.Conv1d(3, 64, kernel_size=1), nn.LeakyReLU(), nn.BatchNorm1d(32),\n","                                 nn.Conv1d(64, 32, kernel_size=1), nn.LeakyReLU(), nn.BatchNorm1d(8),\n","                                 nn.Conv1d(32, 8, kernel_size=1), nn.LeakyReLU(), nn.BatchNorm1d(8),\n","                                 nn.Conv1d(8, 1, kernel_size=1))\n","    def forward(self, inputs):\n","        article_hist, week_hist = inputs[0], inputs[1]\n","        \n","        x = self.artic_embedding(article_hist)\n","        x = F.normalize(x, dim=2)\n","        \n","        # (bs, seq_len, 72582)\n","        x = x@F.normalize(self.artic_embedding.weight).T\n","        \n","        x, indices = x.max(axis=1)\n","        \n","        # (bs, 1, 72582)\n","        x = x.clamp(1e-3, 0.999)\n","        \n","        # inverse sigmoid - converts probabilities to a real valued num.\n","        x = -torch.log(1/x - 1)\n","        \n","        max_week = week_hist.unsqueeze(2).repeat(1, 1, x.shape[-1]).gather(1, indices.unsqueeze(1).repeat(1, week_hist.shape[1], 1))\n","        max_week = max_week.mean(axis=1).unsqueeze(1)\n","        \n","        x = torch.cat([x.unsqueeze(1), max_week, self.artic_likelihood[None, None, :].repeat(x.shape[0], 1, 1)], axis=1)\n","        x = self.top(x).squeeze(1)\n","        \n","        return x\n","    \n","    \n","model = RecSysModel((len(le.classes_), 512))\n","model = model.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["## PyTorch Dataset Class Definition"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T14:58:12.167025Z","iopub.status.busy":"2022-04-24T14:58:12.166728Z","iopub.status.idle":"2022-04-24T14:58:19.483226Z","shell.execute_reply":"2022-04-24T14:58:19.482550Z","shell.execute_reply.started":"2022-04-24T14:58:12.166994Z"},"trusted":true},"outputs":[],"source":["# Create the PyTorch dataset class\n","class RecSysDataset(Dataset):\n","    def __init__(self, main_df, weeks, seq_len, train=True, is_test=False, create_dataset=True):\n","        \n","        # Maximum number of weeks for creating purchase history\n","        self.max_week_hist = 5\n","        \n","        # Create this purchase history dataframe \n","        if create_dataset:\n","            if train:\n","                self.weeks = list(range(1, weeks))\n","                df = pd.concat([self.create_dataset(main_df, w) for w in self.weeks]).reset_index(drop=True)\n","            else:\n","                self.weeks = [0]\n","                df = pd.concat([self.create_dataset(main_df, w) for w in self.weeks]).reset_index(drop=True)\n","\n","            self.df = df.reset_index(drop=True)\n","            self.seq_len = seq_len\n","            self.is_test = is_test\n","        \n","        else:\n","            self.df = main_df.reset_index(drop=True)\n","            self.seq_len = seq_len\n","            self.is_test = is_test\n","            \n","    \n","    def create_dataset(self, df, week):\n","        hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + self.max_week_hist)]\n","        hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n","        hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n","\n","        target_df = df[df[\"week\"] == week]\n","        target_df = target_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n","        target_df.rename(columns={\"article_id\": \"target\"}, inplace=True)\n","        target_df[\"week\"] = week\n","\n","        return target_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","    \n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        # Target is a one-hot vector defining the current article purchased\n","        if self.is_test:\n","            target = torch.zeros(2).float()\n","        else:\n","            target = torch.zeros(len(article_ids)).float()\n","            for t in row.target:\n","                target[t] = 1.0\n","        \n","        # Upto seq_len article_ids can be stored in purchase history\n","        article_hist = torch.zeros(self.seq_len).long()\n","        \n","        # The corresponding weeks for each article in the history \n","        week_hist = torch.ones(self.seq_len).float()\n","        \n","        \n","        if isinstance(row.article_id, list):\n","            if len(row.article_id) >= self.seq_len:\n","                article_hist = torch.LongTensor(row.article_id[-self.seq_len:])\n","                week_hist = (torch.LongTensor(row.week_history[-self.seq_len:]) - row.week)/self.max_week_hist/2\n","            else:\n","                article_hist[-len(row.article_id):] = torch.LongTensor(row.article_id)\n","                week_hist[-len(row.article_id):] = (torch.LongTensor(row.week_history) - row.week)/self.max_week_hist/2\n","                \n","        return article_hist, week_hist, target\n","    \n","RecSysDataset(main_df, 5, 64, train=False, is_test=False, create_dataset=True)[1]"]},{"cell_type":"markdown","metadata":{},"source":["## Training  and Validatation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Model training function\n","def train(model, train_loader, val_loader, criterion, optimizer, epochs):\n","\n","    np.random.seed(SEED)\n","    scaler = torch.cuda.amp.GradScaler()\n","    \n","    for e in range(epochs):\n","        \n","        model.train()\n","        tbar = tqdm(train_loader, file=sys.stdout)\n","        lr = lr_schedule(optimizer, e)\n","        losses = []\n","\n","        for data in tbar:\n","            \n","            # Fetch inputs\n","            inputs = tuple(d.cuda() for d in input_data[:-1])\n","            target = input_data[-1].cuda()\n","\n","            optimizer.zero_grad()\n","            \n","            # Forward pass and calculate loss\n","            with torch.cuda.amp.autocast():\n","                model_out = model(inputs)\n","                loss = criterion(model_out, target) + dice_loss(model_out, target)\n","            \n","            # Update model weights\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            \n","            losses.append(loss.detach().cpu().item())\n","            avg_loss = np.round(100*np.mean(losses), 4)\n","            tbar.set_description(f\"Epoch: {e+1}; lr: {lr}; Loss: {avg_loss}\")\n","            \n","        val_map = validate(model, val_loader)\n","        log_text = f\"Epoch {e+1}\\nTrain Loss: {avg_loss}\\nValidation MAP: {val_map}\\n\"\n","        print(log_text)\n","        \n","    return model\n","\n","# Model validation\n","def validate(model, val_loader, k=12):\n","\n","    model.eval()\n","    tbar = tqdm(val_loader, file=sys.stdout)\n","    maps = []\n","    \n","    with torch.no_grad():\n","        for input_data in tbar:\n","\n","            inputs = tuple(d.cuda() for d in input_data[:-1])\n","            target = input_data[-1].cuda()\n","\n","            model_out = model(inputs)\n","            _, indices = torch.topk(model_out, k, dim=1)\n","\n","            indices = indices.detach().cpu().numpy()\n","            target = target.detach().cpu().numpy()\n","\n","            for i in range(indices.shape[0]):\n","                maps.append(calc_metric(indices[i], target[i]))\n","        \n","    \n","    return np.mean(maps)"]},{"cell_type":"markdown","metadata":{},"source":["### Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dice_loss(y_pred, y_true):\n","\n","    y_pred = y_pred.sigmoid()\n","    intersect = (y_true*y_pred).sum(axis=1)\n","    return 1 - (intersect/(intersect + y_true.sum(axis=1) + y_pred.sum(axis=1))).mean()\n","\n","# Learning rate schedule\n","def lr_schedule(optimizer, epoch):\n","\n","    if epoch < 1:\n","        lr = 5e-5\n","    elif epoch < 6:\n","        lr = 1e-3\n","    elif epoch < 9:\n","        lr = 1e-4\n","    else:\n","        lr = 1e-5\n","\n","    for param in optimizer.param_groups:\n","        param['lr'] = lr\n","        \n","    return lr\n","\n","# Adam Optimizer definition\n","def get_optimizer(model):\n","    \n","    optim = torch.optim.Adam(filter(lambda param: param.requires_grad, model.parameters()), lr=3e-4, betas=(0.9, 0.999),eps=1e-08)\n","    return optim\n","\n","# Calculate mAP metric on the validation set\n","def calc_metric(topk_preds, target_array, k=12):\n","    \n","    map_met = []\n","    tp, fp = 0, 0\n","    for pred in topk_preds:\n","        if target_array[pred]:\n","            tp += 1\n","            map_met.append(tp/(tp + fp))\n","        else:\n","            fp += 1\n","            \n","    return np.sum(map_met) / min(k, target_array.sum())"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T20:23:46.441581Z","iopub.status.busy":"2022-04-23T20:23:46.441188Z","iopub.status.idle":"2022-04-23T22:15:53.530932Z","shell.execute_reply":"2022-04-23T22:15:53.530002Z","shell.execute_reply.started":"2022-04-23T20:23:46.441543Z"},"trusted":true},"outputs":[],"source":["MODEL_NAME = \"exp001\"\n","\n","# Dataloaders for training and validation datasets\n","val_dataset = RecSysDataset(main_df, 0, SEQ_LEN, train=False, is_test=False)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n","                          pin_memory=False, drop_last=False)\n","\n","train_dataset = RecSysDataset(train_df, weeks=5, seq_len=SEQ_LEN, train=True, is_test=False)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n","                          pin_memory=False, drop_last=True)\n","\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = get_optimizer(model)\n","model = train(model, train_loader, val_loader, criterion, optimizer, epochs=10)"]},{"cell_type":"markdown","metadata":{},"source":["## Finetune along with Validation Data"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-23T22:15:53.536114Z","iopub.status.busy":"2022-04-23T22:15:53.534629Z","iopub.status.idle":"2022-04-24T00:07:06.027688Z","shell.execute_reply":"2022-04-24T00:07:06.026869Z","shell.execute_reply.started":"2022-04-23T22:15:53.536067Z"},"trusted":true},"outputs":[],"source":["train_dataset = RecSysDataset(train_df[train_df[\"week\"] < 4].append(val_df), 5, SEQ_LEN, train=True, is_test=False, create_dataset=False)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n","                          pin_memory=False, drop_last=True)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = get_optimizer(model)\n","\n","model = train(model, train_loader, val_loader, criterion, optimizer, epochs=10)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv('sample_submission.csv').drop(\"prediction\", axis=1)\n","print('Test dataframe shape: ', test_df.shape)\n","test_df.head()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T00:07:10.418001Z","iopub.status.busy":"2022-04-24T00:07:10.415818Z","iopub.status.idle":"2022-04-24T00:07:18.011370Z","shell.execute_reply":"2022-04-24T00:07:18.010604Z","shell.execute_reply.started":"2022-04-24T00:07:10.417959Z"},"trusted":true},"outputs":[],"source":["# For inference, gather article purchase history (past 5 weeks) for each customer id on sample_submission.csv\n","# Use this to make predictions\n","def create_test_dataset(test_df):\n","    max_week_hist = 5\n","    \n","    week = -1\n","    test_df[\"week\"] = week \n","    hist_df = main_df[(main_df[\"week\"] > week) & (main_df[\"week\"] <= week + max_week_hist)]\n","    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n","    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n","    \n","    return test_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n","\n","test_df = create_test_dataset(test_df)\n","test_df.head()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T00:07:18.095636Z","iopub.status.busy":"2022-04-24T00:07:18.095275Z","iopub.status.idle":"2022-04-24T00:52:30.446042Z","shell.execute_reply":"2022-04-24T00:52:30.445127Z","shell.execute_reply.started":"2022-04-24T00:07:18.095598Z"},"trusted":true},"outputs":[],"source":["test_ds = RecSysDataset(test_df, 0, SEQ_LEN, train=False, is_test=True, create_dataset=False)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n","                          pin_memory=False, drop_last=False)\n","\n","# Inference pipeline\n","def inference(model, loader, k=12):\n","    model.eval()\n","    tbar = tqdm(loader, file=sys.stdout)\n","    \n","    prediction_list = []\n","    \n","    with torch.no_grad():\n","        for data in tbar:\n","            inputs = tuple(d.cuda() for d in input_data[:-1])\n","            target = input_data[-1].cuda()\n","\n","            model_out = model(inputs)\n","            _, indices = torch.topk(model_out, k, dim=1)\n","\n","            indices = indices.detach().cpu().numpy()\n","            target = target.detach().cpu().numpy()\n","\n","            for i in range(indices.shape[0]):\n","                prediction_list.append(\" \".join(list(le.inverse_transform(indices[i]))))\n","        \n","    \n","    return prediction_list\n","\n","test_df[\"prediction\"] = inference(model, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Save Outputs"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-04-24T00:52:30.448353Z","iopub.status.busy":"2022-04-24T00:52:30.447894Z","iopub.status.idle":"2022-04-24T00:52:42.525163Z","shell.execute_reply":"2022-04-24T00:52:42.524215Z","shell.execute_reply.started":"2022-04-24T00:52:30.448307Z"},"trusted":true},"outputs":[],"source":["test_df.to_csv(\"output.csv\", index=False, columns=[\"customer_id\", \"prediction\"])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
